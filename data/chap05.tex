\chapter{有额外信息的随机块模型}
\label{chap:sbmsi}
在本章中，我们首先引入我们研究的有额外信息的随机块模型
的定义。然后研究它的精确恢复的条件以及实现其精确恢复的半正定规划算法。

\section{精确恢复条件}
\label{sec:sbmsi_exact_recovery_condtion}
\newacronym{acr:sbmsi}{SBMSI}{Stochastic Block Model with side information}
带有额外信息的随机块模型（\gls{acr:sbmsi}）
是 \ref{sec:sbm} 节介绍的
SSBM 模型的推广。这里我们在2个社群的随机块模型的
基础上引入额外信息。
除了图 $Z$ 和节点标签 $X$ 外，
第 $i$  个节点 ($1\leq i \leq n$) 
还含有 $m=\lceil \gamma \log n \rceil $ 个相关的数据样本 
$Y^{i}_{j}$, $j\in \{1,\ldots,m\}$。
这里，我们要求 $\gamma$ 是一个正整数。
若 $X_i=1$
这些样本 i.i.d. 采样自分布 $P_0$，若  $X_i=-1$ 则采样自 $P_1$。
注意到 给定 标签 $X_i$，
对于 $i\in\{1,\ldots,n\}$，数据样本 $Y^{i}_{j}$, $j\in \{1,\ldots,m\}$ 与 $\{Z_{i,j}\}_{1\le i<j\le n}$ 独立。
 因此，在标签 $X$ 给定的情况下，
  $(\{Z_{i,j}\}_{1\le i<j\le n},\{Y^i_{j}\}_{1\le i\le n,1\le j\le m})$ 的联合分布为  
\begin{align}\label{eq:lh}
    &P(y=\{y^i_{j}\}_{1\le i\le n,1\le j\le m},z=\{z_{i,j}\}_{1\le i<j\le n}| (x_1,\ldots,x_n)) \nonumber\\
    &= \prod_{1\le i,j\le n}P(z_{i,j}|x_i,x_j)\prod_{i=1}^n \prod_{j=1}^m P(y^i_j|x_i), 
\end{align}
其中$\prod_{1\le i,j\le n}P(z_{i,j}|x_i,x_j)$ 可写成
\eqref{eq:mle_sibm} 式的形式。对于$P(z_{i,j}|x_i,x_j)$，其具体定义为
\begin{equation*}
    P  (z_{i,j}=1|x_i,x_j) = \begin{cases}
        p & \text{if } x_i = x_j \\
        q & \text{if } x_i\ne x_j
    \end{cases},
\end{equation*}
并且
\begin{equation*}
    P(y^i_j|x_i) = \begin{cases}
        P_0(y^i_j) & x_i = 1 \\
        P_1(y^i_j) & x_i = -1
    \end{cases}
\end{equation*}
 $P(\{x^i_{j}\}_{1\le i\le n,1\le j\le m},\{z_{i,j}\}_{1\le i<j\le n}| y_1,\ldots,y_n)$ 
 的条件分布 依赖于
 $n,m,p, q, P_0$ 和 $P_1$ 这些参数，因此我们把 SBMSI 写成 $\SBMSI(n,m,p,q,P_0,P_1)$ 的形式。
 给定由 $\SBMSI(n,m,p,q,P_0,P_1)$ 生成的图$Z$和数据$Y$, 我们的目标是恢复 出$X$。
 
 在有额外信息的条件下，
 我们仍使用能否精确恢复这一指标来衡量
 社群发现的可行性。与定义\ref{def:SSBMR}类似，
 若存在以$Z,Y$为输入变量的算法 $\hat{X}=\hat{X}(Z,Y)$
 使得当$n$增大时，
 错误概率
 $P_e:=P(\hat{Y} \neq Y)$ 趋向于 $0$，则称
 精确恢复可实现。

 \citet{abbe2015community}一文的3.2小节指出了一般SBM模型的精确恢复条件与多维泊松分布的关系。
在$k=2$的情形，我们要用到2维泊松分布，因此这里对其做一个简单的介绍。
有多种方式推广一维的泊松分布，这里我们使用的是如下定义：

\begin{definition}
    称 $(X,Y)$ 服从参数为 $(\lambda, \mu)$ 的二维泊松分布，
    如果其概率质量函数为：
    \begin{equation}
        P(X=k, Y=j) = \frac{\lambda^k \mu^j}{k! j!}
        e^{-\lambda - \mu}, k,j=0,1,\dots
    \end{equation}
    记为 $(X,Y) \sim \Pois(\lambda, \mu)$。
\end{definition}

有了2维泊松分布的定义式，我们下面给出
SBMSI 模型精确恢复的条件：
\begin{theorem}\label{thm:Pe}
    定义$I_+$ 为联合分布 $\textrm{Pois}(\frac{a}{2},\frac{b}{2})\times \underbrace{P_0 \times \dots \times P_0}_{\gamma}$
    与 $\textrm{Pois}(\frac{b}{2}, \frac{a}{2})\times \underbrace{P_1 \times \dots \times P_1}_{\gamma}$ 
    之间的切尔诺夫信息。    
    对于 $\SBMSI(n,m,p=a\frac{\log n}{n},q=b\frac{\log n}{n},P_0,P_1)$，
    如果 $I_+>1$，则精确恢复可实现；
    如果 $I_+ < 1$，
    则错误概率 $P_e$ 趋近于 $1$，精确恢复不可实现。
\end{theorem}
对于一般的情形，SBMSI 模型精确恢复的 临界值 $I_+$ 并没有
显示表达式，但我们可以从切尔诺夫信息的定义出发得到 $I_+$ 的简化表达式，
由下述引理 \ref{lem:I_plus_expression} 给出。
该表达式更容易计算，且在定理\ref{thm:Pe}的证明中也会用到。

\begin{lemma}\label{lem:I_plus_expression}
\begin{align}\label{equation:I+}
    I_+ &=\frac{\lambda^* }{2} (a^{1-\lambda^* }b^{\lambda^* } -
    b^{1-\lambda^* }a^{\lambda^* })\log\frac{b}{a}+\frac{a+b}{2}\notag \\
    &-\frac{1}{2}(a^{1-\lambda^*}b^{\lambda^*} +
    b^{1-\lambda^* }a^{\lambda^* })+\gamma D(P_{\lambda^* }||P_0) 
	\end{align}
	其中
	\begin{equation}\label{eq:lambda}
    \lambda^* = \arg\min_{\lambda} \left[a^{1-\lambda}b^{\lambda} +
    b^{1-\lambda}a^{\lambda} + 2\gamma \log
    \left(\sum_{x\in \mathcal{X}}P^{1-\lambda}_0(x) P^{\lambda}_1(x)
    \right)
    \right]
\end{equation}
而 $P_{\lambda}$ 由\eqref{eq:P_lambda_x} 式定义。
\end{lemma}



下面我们给出 SBMSI 模型的精确恢复条件
	
\begin{theorem}\label{thm:Pe}
    对于 $\SBMSI(n,m,p=a\frac{\log n}{n},q=b\frac{\log n}{n},P_0,P_1)$，若条件
    \begin{equation}\label{eq:positive_condition}
        I_+ > 1,
    \end{equation}
    满足，则模型精确可恢复；
    反之，若 $I_+ < 1$,  错误概率 $P_e$ 趋向于 $1$。
\end{theorem}
相比于 \citet{abbe17sideinfo} 中定理4的结论，
这里我们限制了类别数为2等条件但允许采样数量为
$\lceil \gamma \log n \rceil $。

当 $\gamma=0$ 时，由\eqref{eq:lambda}式 可得 $\lambda^*=\frac{1}{2}$，
则 $I_+=\frac{1}{2}(\sqrt{a}-\sqrt{b})^2$。
由定理\ref{thm:Pe}，$\sqrt{a}-\sqrt{b} > \sqrt{2}$
时可精确恢复。定理\ref{thm:Pe}退化到
定理\ref{thm:sbm2_phase_transition}。


\section{误差衰减速率}
在\ref{sec:sbmsi_exact_recovery_condtion}节的基础上，
我们讨论一种更强的要求，即恢复算法将SSBM各社群大小相等的条件考虑在内。
类似定理\ref{thm:error_rate}，
在这种情况下，利用带约束的最大似然估计，我们可以得到恢复算法更快的误差衰减速率。
%{eq:mle_sibm}

具体而言，在\eqref{eq:lh}式中极小化$x_i$可得到如下优化问题：
\begin{align}
    \hat{X} &= \arg\max_x\ P(y,z|X=x) \notag \\
    \textrm{s.t.} \ & x_i \in \{\pm 1\}, \sum_{i=1}^n x_i=0 \label{eq:mle}
\end{align}
类似\eqref{eq:hatX_double_prime}式，
由\eqref{eq:mle}给出的估计量 $\hat{X}$，
是在有约束的参数空间的最大似然估计量。
如果我们修改定义\ref{def:SSBM}中真实标签$X$的生成方式，
使得$X_1, \dots, X_n$ i.i.d. $\sim \textrm{Bernoulli}(\frac{1}{2})$，
则只能使用不带约束的 ML 估计量进行社群发现。

当SSBM 的 $p,q$ 参数以 $O(\frac{ \log n}{n})$
的速率衰减时，我们有如下定理：
\begin{theorem}\label{thm:Pe_new}
    令
    $\gamma = \frac{ m}{\log n}$
    为一常数。
    若
    $p = a \log n /n$ 且 $q = b \log n / n$, 使用最大似然估计量 \eqref{eq:mle}
    进行社群发现，
    若
    \begin{equation}\label{eq:positive_condition_new}
    \gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2 > 0
    \end{equation}
    则精确恢复的误差的上界为：
    \begin{equation}\label{eq:PeMain}
    P_e \leq (\frac{1}{4}+o(1)) n^{-\left(\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2 + o(1)\right) }
    \end{equation}
    如果下述条件满足，
    \begin{align}
    (\sqrt{a}-\sqrt{b})^2-2 
    > 3a^{1/3}b^{1/3}(a^{1/6}-b^{1/6})^2\label{eq:oneC}
    \end{align}
    则精确恢复的误差的下界为：
    \begin{equation}\label{eq:PeMainL}
    P_e \geq (\frac{1}{4}+o(1)) n^{-\left(\gamma D_{1/2}(p_0||p_1) + (\sqrt{a} - \sqrt{b})^2-2 + o(1)\right)}
    \end{equation}
\end{theorem}
