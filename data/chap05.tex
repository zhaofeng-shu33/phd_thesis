\chapter{有额外信息的随机块模型}
\label{chap:sbmsi}
在本章中，我们首先引入我们研究的有额外信息的随机块模型
的定义。然后研究它的精确恢复的条件以及实现其精确恢复的半正定规划算法。

\section{精确恢复条件}
\label{sec:sbmsi_exact_recovery_condtion}
\newacronym{acr:sbmsi}{SBMSI}{Stochastic Block Model with side information}
带有额外信息的随机块模型（\gls{acr:sbmsi}）
是 \ref{sec:sbm} 节介绍的
SSBM 模型的推广。这里我们在2个社群的随机块模型的
基础上引入额外信息。
除了图 $Z$ 和节点标签 $X$ 外，
第 $i$  个节点 ($1\leq i \leq n$) 
还含有 $m=\lceil \gamma \log n \rceil $ 个相关的数据样本 
$Y^{i}_{j}$, $j\in \{1,\ldots,m\}$。
这里，我们要求 $\gamma$ 是一个正整数。
若 $X_i=1$
这些样本 i.i.d. 采样自分布 $P_0$，若  $X_i=-1$ 则采样自 $P_1$。
这里假设 $P_0, P_1$ 两个分布函数均为定义在字母集$\mathcal{Y}$上的离散分布。
注意到 给定 标签 $X_i$，
对于 $i\in\{1,\ldots,n\}$，数据样本 $Y^{i}_{j}$, $j\in \{1,\ldots,m\}$ 与 $\{Z_{i,j}\}_{1\le i<j\le n}$ 独立。
 因此，在标签 $X$ 给定的情况下，
  $(\{Z_{i,j}\}_{1\le i<j\le n},\{Y^i_{j}\}_{1\le i\le n,1\le j\le m})$ 的联合分布为  
\begin{align}\label{eq:lh}
    &P(y=\{y^i_{j}\}_{1\le i\le n,1\le j\le m},z=\{z_{i,j}\}_{1\le i<j\le n}| (x_1,\ldots,x_n)) \nonumber\\
    &= \prod_{1\le i,j\le n}P(z_{i,j}|x_i,x_j)\prod_{i=1}^n \prod_{j=1}^m P(y^i_j|x_i), 
\end{align}
其中$\prod_{1\le i,j\le n}P(z_{i,j}|x_i,x_j)$ 可写成
\eqref{eq:mle_sibm} 式的形式。对于$P(z_{i,j}|x_i,x_j)$，其具体定义为
\begin{equation*}
    P  (z_{i,j}=1|x_i,x_j) = \begin{cases}
        p & \text{if } x_i = x_j \\
        q & \text{if } x_i\ne x_j
    \end{cases},
\end{equation*}
并且
\begin{equation*}
    P(y^i_j|x_i) = \begin{cases}
        P_0(y^i_j) & x_i = 1 \\
        P_1(y^i_j) & x_i = -1
    \end{cases}
\end{equation*}
 $P(\{x^i_{j}\}_{1\le i\le n,1\le j\le m},\{z_{i,j}\}_{1\le i<j\le n}| y_1,\ldots,y_n)$ 
 的条件分布 依赖于
 $n,m,p, q, P_0$ 和 $P_1$ 这些参数，因此我们把 SBMSI 写成 $\SBMSI(n,m,p,q,P_0,P_1)$ 的形式。
 给定由 $\SBMSI(n,m,p,q,P_0,P_1)$ 生成的图$Z$和数据$Y$, 我们的目标是恢复 出$X$。
 
 在有额外信息的条件下，
 我们仍使用能否精确恢复这一指标来衡量
 社群发现的可行性。与定义\ref{def:SSBMR}类似，
 若存在以$Z,Y$为输入变量的算法 $\hat{X}=\hat{X}(Z,Y)$
 使得当$n$增大时，
 错误概率
 $P_e:=P(\hat{Y} \neq Y)$ 趋向于 $0$，则称
 精确恢复可实现。

 \citet{abbe2015community}一文的3.2小节指出了一般SBM模型的精确恢复条件与多维泊松分布的关系。
在$k=2$的情形，我们要用到2维泊松分布，因此这里对其做一个简单的介绍。
有多种方式推广一维的泊松分布，这里我们使用的是如下定义：

\begin{definition}
    称 $(X,Y)$ 服从参数为 $(\lambda, \mu)$ 的二维泊松分布，
    如果其概率质量函数为：
    \begin{equation}
        P(X=k, Y=j) = \frac{\lambda^k \mu^j}{k! j!}
        e^{-\lambda - \mu}, k,j=0,1,\dots
    \end{equation}
    记为 $(X,Y) \sim \Pois(\lambda, \mu)$。
\end{definition}

有了2维泊松分布的定义式，我们下面给出
SBMSI 模型精确恢复的条件：
\begin{theorem}\label{thm:Pe}
    定义$I_+$ 为联合分布 $\textrm{Pois}(\frac{a}{2},\frac{b}{2})\times \underbrace{P_0 \times \dots \times P_0}_{\gamma}$
    与 $\textrm{Pois}(\frac{b}{2}, \frac{a}{2})\times \underbrace{P_1 \times \dots \times P_1}_{\gamma}$ 
    之间的切尔诺夫信息。    
    对于 $\SBMSI(n,m=\lceil \gamma \log n \rceil, p=a\frac{\log n}{n},q=b\frac{\log n}{n},P_0,P_1)$，
    如果 $I_+>1$，则精确恢复可实现；
    如果 $I_+ < 1$，
    则错误概率 $P_e$ 趋近于 $1$，精确恢复不可实现。
\end{theorem}

相比于 \citet{abbe17sideinfo} 中定理4的结论，
这里我们限制了类别数为2等条件，但允许采样数量 $m$ 为
$\lceil \log n \rceil $ 的整数倍。

对于一般的情形，SBMSI 模型精确恢复的 临界值 $I_+$ 并没有
显示表达式，但我们可以从切尔诺夫信息的定义出发得到 $I_+$ 的简化表达式，
由下述引理 \ref{lem:I_plus_expression} 给出。
该表达式更容易计算，且在定理\ref{thm:Pe}的证明中也会用到。

\begin{lemma}\label{lem:I_plus_expression}
\begin{align}\label{equation:I+}
    I_+ &=\frac{\lambda^* }{2} (a^{1-\lambda^* }b^{\lambda^* } -
    b^{1-\lambda^* }a^{\lambda^* })\log\frac{b}{a}+\frac{a+b}{2}\notag \\
    &-\frac{1}{2}(a^{1-\lambda^*}b^{\lambda^*} +
    b^{1-\lambda^* }a^{\lambda^* })+\gamma D(P_{\lambda^* }||P_0) 
	\end{align}
	其中
	\begin{equation}\label{eq:lambda}
    \lambda^* = \arg\min_{\lambda} \left[a^{1-\lambda}b^{\lambda} +
    b^{1-\lambda}a^{\lambda} + 2\gamma \log
    \left(\sum_{x\in \mathcal{X}}P^{1-\lambda}_0(x) P^{\lambda}_1(x)
    \right)
    \right]
\end{equation}
而 $P_{\lambda}$ 类似\eqref{eq:P_lambda_x} 式定义，为
\begin{equation}\label{eq:P_lambda_0_1}
    P_{\lambda}(x) = \frac{P_0^{1-\lambda}(x) P_1^{\lambda} (x)}
    {\sum_{x \in \mathcal{X}}P_0^{1-\lambda}(x) P_1^{\lambda} (x)}        
\end{equation}

\end{lemma}




当 $\gamma=0$ 时，由\eqref{eq:lambda}式 可得 $\lambda^*=\frac{1}{2}$，
则 $I_+=\frac{1}{2}(\sqrt{a}-\sqrt{b})^2$。
由定理\ref{thm:Pe}，$\sqrt{a}-\sqrt{b} > \sqrt{2}$
时可精确恢复。定理\ref{thm:Pe}退化到
定理\ref{thm:sbm2_phase_transition}。


\section{误差衰减速率}
在\ref{sec:sbmsi_exact_recovery_condtion}节的基础上，
我们讨论一种更强的要求，即恢复算法将SSBM各社群大小相等的条件考虑在内。
类似定理\ref{thm:error_rate}，
在这种情况下，利用带约束的最大似然估计，我们可以得到恢复算法更快的误差衰减速率。
%{eq:mle_sibm}

具体而言，在\eqref{eq:lh}式中极小化$x_i$可得到如下优化问题：
\begin{align}
    \hat{X} &= \arg\max_x\ P(y,z|X=x) \notag \\
    \textrm{s.t.} \ & x_i \in \{\pm 1\}, \sum_{i=1}^n x_i=0 \label{eq:mle}
\end{align}
类似\eqref{eq:hatX_double_prime}式，
由\eqref{eq:mle}给出的估计量 $\hat{X}$，
是在有约束的参数空间的最大似然估计量。
如果我们修改定义\ref{def:SSBM}中真实标签$X$的生成方式，
使得$X_1, \dots, X_n$ i.i.d. $\sim \textrm{Bernoulli}(\frac{1}{2})$，
则只能使用不带约束的 ML 估计量进行社群发现。

根据 SBMSI 中的 SSBM 的模型参数 $p,q$ 随 $n$的变化规律，我们下面分两种情况讨论。
\subsection{稀疏的 SSBM }
\newglossaryentry{not:big_o}
{
  type=notation,
  name={$f(n)=O(g(n))$},
  description={大O符号，表示存在常数$C$和正整数$N$，使得$f(n)\leq C g(n)$ 对于$n\geq N$均成立}
}
当SSBM 的 $p,q$ 参数以 $O(\frac{ \log n}{n})$
的速率衰减时，我们有如下定理：
\begin{theorem}\label{thm:Pe_new}
    令
    $\gamma = \frac{ m}{\log n}$
    为一常数。
    若
    $p = \frac{a \log n }{n} $ 且 $q = 
    \frac{b \log n } {n}$, 使用最大似然估计量 \eqref{eq:mle}
    进行社群发现，
    若
    \begin{equation}\label{eq:positive_condition_new}
    \gamma D_{1/2}(P_0||P_1) +
     (\sqrt{a} - \sqrt{b})^2-2 > 0
    \end{equation}
    则精确恢复的误差的上界为：
    \begin{equation}\label{eq:PeMain}
    P_e \leq (\frac{1}{4}+o(1)) n^{-\left(\gamma D_{1/2}(P_0||P_1) + (\sqrt{a} - \sqrt{b})^2-2 + o(1)\right) }
    \end{equation}
    如果下述条件满足，
    \begin{align}
    (\sqrt{a}-\sqrt{b})^2-2 
    > 3a^{1/3}b^{1/3}(a^{1/6}-b^{1/6})^2\label{eq:oneC}
    \end{align}
    则精确恢复的误差的下界为：
    \begin{equation}\label{eq:PeMainL}
    P_e \geq (\frac{1}{4}+o(1)) n^{-\left(\gamma D_{1/2}(P_0||P_1) + (\sqrt{a} - \sqrt{b})^2-2 + o(1)\right)}
    \end{equation}
\end{theorem}

由定理 \ref{thm:Pe_new}
可看出，误差界的幂次有两项组成，
一项 $\gamma D_{1/2}(P_0||P_1) $ 
和雷尼散度（在\eqref{eq:renyi_divergence} 定义）
有关，另一项 $ (\sqrt{a} - \sqrt{b})^2-2 $
和 SSBM 的精确恢复条件有关。

定理
\ref{thm:Pe_new} 告诉我们
额外信息  $Y$  增大了 误差 $P_e$ 的衰减速率，
增大的程度可用信息量 $\gamma D_{1/2}(P_0||P_1)$ 
来刻画。
若额外的参数条件 \eqref{eq:oneC}式满足，
\eqref{eq:positive_condition_new} 式也满足，此时
误差衰减速率为：
$$
-\lim_{n\to \infty} \frac{\log P_e}{\log n}
= \gamma D_{1/2}(P_0||P_1) + (\sqrt{a} - \sqrt{b})^2-2
$$
此外，当 $P_0=P_1$ 时，
定理 \ref{thm:Pe_new} 
给出了用带约束的最大似然算法
对SSBM 做社群发现
的误差衰减速率，总结为如下推论：
\begin{corollary}\label{cor:sbm}
考虑
$\SSBM(n,\frac{a\log n}{n}, \frac{b \log n}{n}, 2)$，
对于\eqref{eq:mle}式中的ML算法， 
若 条件 \eqref{eq:oneC} 满足，
则错误概率 $P_e$ 满足
\begin{equation}\label{eq:cor}
\lim_{n\to \infty} \frac{\log P_e}{\log n} =2-(\sqrt{a} - \sqrt{b})^2
\end{equation}

\end{corollary}
相比于定理\ref{thm:error_rate}中的第2条，
这里我们推导出了误差下界的表达式 \eqref{eq:PeMainL}，
因而得到了紧的误差误差速率 \eqref{eq:cor}。

下面我们讨论本节的误差衰减速率项
$\gamma D_{1/2}(P_0||P_1) +
(\sqrt{a} - \sqrt{b})^2$与上一节
精确恢复条件中的$I^+$ 的关系。我们有
如下定理
\begin{theorem}\label{thm:I_plus_relation}
    设$I_+$ 在 式 \eqref{equation:I+}
    中定义，则
    \begin{equation}
        I_+ \geq \frac{1}{2}
        (\sqrt{a} - \sqrt{b})^2 +
        \frac{\gamma}{2} D_{1/2}(P_0||P_1)
    \end{equation}
\end{theorem}
	由定理\ref{thm:I_plus_relation}
    可知，随着  $\gamma$ 的增大, 即每个节点额外采样数量的增多， $I_+$
    的下界增大。因此，更多的样本有助于满足精确恢复的条件 $I_+>1$。 

\subsection{稠密的 SSBM}
当 SSBM 的 $p,q$ 参数是常数时，我们有如下定理：
\begin{theorem}\label{thm:constant}
	令 $\gamma = \frac{m}{n}$ 为一常数。
    若 $p,q$ 为常数，使用最大似然估计量 \eqref{eq:mle}
    进行社群发现，
	精确恢复的误差指数
    为
    \newglossaryentry{not:bern}
{
  type=notation,
  name={$\Bern(p)$},
  description={参数为$p$的伯努利分布}
}
	\begin{equation}\label{eq:dense_error_exponent}
	-\lim_{n\to \infty} \frac{1}{n}\log P_e = 
     \gamma D_{1/2}(P_0 || P_1) + D_{1/2}(\Bern(p)||\Bern(q))
	\end{equation} 
\end{theorem}

由定理 \ref{thm:constant} 可知，精确恢复误差指数下降。
当 $\gamma=0$ 时，定理 \ref{thm:constant} 指出
$D_{1/2}(\Bern(p)||\Bern(q))$
是精确恢复的误差指数。
因为
弱恢复与强恢复相差一个多项式的数量级，
$D_{1/2}(\Bern(p)||\Bern(q))$ 也是弱恢复的误差指数
\cite{zhang2016}。
除此之外， 假设 $\gamma$ 为整数，
误差指数可视为 
联合分布
$\underbrace{P_0\times \dots \times P_0}_{\gamma} \times \Bern(p)$
和 $\underbrace{P_1\times \dots \times P_1}_{\gamma} \times \Bern(q)$
之间的雷尼散度。
由独立性条件，
我们可以把这一散度分解成如\eqref{eq:dense_error_exponent}
所示的两项求和的形式。
进一步的， 定理 \ref{thm:constant}
的结果要求
$m$ 和 $n$ 量阶相同。
当 $m=o(n)$ 时，
图中边的信息占主导
地位；
而当 $m=\omega(n)$时，
每个节点的额外信息占
主导地位，此时所研究的问题变成$n$个独立的假设检验问题。
\newglossaryentry{not:small_omega}
{
  type=notation,
  name={$f(n)=\omega(g(n))$},
  description={$\lim_{n\to \infty} \frac{f(n)}{g(n)} = +\infty$}
}

\section{实现精确恢复的SDP算法}
本节我们给出实现关于SBMSI的精确恢复的SDP算法。

%给定节点标签$X$，
%令 $S_1(X)$ 和 $S_2(X)$ 分别表示 标签为1和-1的节点集。
可以验证， 在 $X\in\{\pm 1\}^n$ 的取值范围内
极大化 关于
$(Y,Z)$ 的对数似然函数 式 \eqref{eq:lh} 等价于求解下面的优化问题
\begin{align}
    \max_{v}\, & h^T v + \frac{1}{4}v^T B v \notag \\
    \textrm{s.t. }\, & \mathbf{1}_n^T v = 0 \text{ 且 } v_i \in \{\pm 1 \} \label{eq:matrix_mle}
\end{align}
其中 $h$ 是 n 长的向量，第i个元素为$h_i = \frac{1}{\log \frac{p(1-q)}{q(1-p)}}\sum_{j=1}^m \log \frac{P_0(y^i_{j})}{P_1(y^i_{j})}, i\in [n]$，
而  $n\times n $ 的矩阵 $B$ 在 式\eqref{eq:def_B_sdp} 中定义。

令 $x^*$ 是 \eqref{eq:matrix_mle} 的最优值，
且 $V^*=(1,x^*)(1,x^*)^T$， 
其中 $(1,x^*)$ 是一个
将 $1$ 和 $x^*$ 连接起来的
$(n+1)$ 维的向量。
	则 式\eqref{eq:matrix_mle} 中目标函数的
    最优值等于 $\frac{1}{2} \langle \widetilde{B}, V^* \rangle$, 其中
	\begin{equation}\label{eq:B_lambda_def}
		\widetilde{B} = \begin{pmatrix} 0 & h^T  \\ h  & \frac{1}{2}B \end{pmatrix}.
	\end{equation}
	我们将证明 $V^*$ 是如下问题的唯一最优解：
	\begin{align}
		\max_V\, \langle \tilde{B}, V \rangle  &\notag\\
		\textrm{ s.t. }\, V_{ii} &= 1, \notag\\
		V &\succeq 0, \notag\\
\sum_{j=2}^{n+1} (V_{ij} + V_{ji})& = 0,~\forall\, i\in\{1,\ldots,n+1\}. \label{eq:sdp}
	\end{align}
	注意到在 式\eqref{eq:sdp} 的最后一行
    我们用了 $n+1$ 个约束条件 来描述标签 平衡的条件。 这些约束条件在证明$V^*$是式\eqref{eq:sdp}唯一的最优解时起到关键作用。
    该结论可总结为下述定理。
	%When $m=0$ and $\kappa=1$,
	%the SDP is exactly the one Abbe considered in \cite{abbe2015exact}.
	%When $m=0$ and $\kappa=\frac{a-b}{\log a/b}\frac{\log n}{n}$, the SDP is
	%the same as the one in Theorem 3 of \cite{Hajek16}.
	% The theoretical guarantee for the relaxation form in \eqref{eq:sdp} is summarized
	% in the following theorem:
	\begin{theorem}\label{thm:sdp}
        设 $\hat{V}$ 是优化问题 \eqref{eq:sdp} 的解，
        $x^*$ 是真实的标签向量。
		若 $I_+ > 1$, 则
        $\lim_{n\to\infty} P(\hat{V}=V^*)=1$，其中
		$V^*=(1,x^*)(1,x^*)^T$。
	\end{theorem}
    该定理表明，只要满足恢复条件$I^+>1$，我们的SDP松弛方法可以高概率地实现原社群的精确恢复。