% !TeX root = ../thuthesis-example.tex

\chapter{背景知识}\label{chp:knowledge}
本章对社团和信息论度量的相关概念、社团发现算法与
相关的理论研究成果进行简要介绍，作为我们
后续研究的基础。
\section{社团发现的数学模型}\label{sec:community_detection}
\newglossaryentry{cd}{name=社团发现, description={Community Detection}}

\newglossaryentry{graph}{name=图, description={Graph}}
在网络中的社团发现中，
通常将网络建模成\gls{graph}(\glsdesc{graph})结构。
而社团没有严格的数学定义，
通常认为社团内部连边紧密、不同社团间连边稀疏。\gls{cd}(\glsdesc{cd})，也称社团检测，
即是在图结构的数据中上寻找特定的社团结构。
%社团根据其属性
%是否随时间变化可分为静态社团和动态社团，本文研究的是静态社团，
%它可以用
%静态图来建模，以下简称图。
一个图结构$G$ 由节点集 $V$ 和边的集合 $E$ 组成，记为 $G(V,E)$。
根据边是否有方向可将图分为有向图和无向图。无向图可以看成是每条边
有两个方向，从而针对有向图设计的社团发现算法也可以用到无向图上面来。
此外，根据边是否有权值可将图分为有权图和无权图。有权图中节点 $i$
和节点 $j$ 之间的权值可以用 $w_{ij}$ 表示。若有权图有方向，
则 $w_{ij}$ 可以不等于 $w_{ji}$。
无向图可以看成是每条边
的权值是1，从而针对有权图设计的社团发现算法也可以用到无权图上面来。

社团发现算法根据网络是否随时间变化与社团之间是否重叠可进行粗略的分类，
本文研究的是静态网络中非重叠的社团发现算法，即$G$不随时间变化且
社团之间不重叠。
下面，我们首先介绍一个图分割算法，
可将$V$可以分为互不重叠的若干子集。

\section{基于图强度的图分割算法}\label{sec:graph_strength}
\newglossaryentry{graph_strength}{name=图强度, description={Strength of a graph}}
根据\citet{fortunato2010community} 对社团发现算法的分类，图分割可用于社团发现，
该类别属于20世纪末提出的传统算法。
%不同的社团发现算法根据自己的标准寻找社团结构，这给一般意义上的比较造成了困难。
在本节中我们介绍的图分割算法采用的度量标准叫做图强度。
\gls{graph_strength}这一概念最早由 Cunningham 提出 \cite{cunningham1985optimal}，
它的推广包括最小化平均误差\cite{mac}和多变量互信息\cite{chan2016ic}等，
其计算方法具有相似性。
图强度可以定义在无权图或有权图上，
在这里我们针对带权的有向图来介绍这一度量和基于图强度的图分割算法。

给定有向图$G(V,E)$，其图强度定义为
\begin{equation}\label{eq:IP}
  \lambda_1 := \min_{\P \in \Pi'}\frac{ f[\P] }{  \abs{\P} - 1 } 
\end{equation}
在式 \eqref{eq:IP} 中，
\newglossaryentry{not:partition}
{
  type=notation,
  name={$\P$},
  description={$V$的一个分割，
  也即  $P=\{C_1, \dots, C_k\},
  \cup_{i=1}^k C_i=V$}
}
$\P$ 表示 \glsdesc{not:partition}。
$\abs{\mathcal{P}}$ 表示集合 $\P$ 中元素的个数。
$\Pi$ 是 $V$ 上所有分割的集合而 $\Pi'=\Pi\backslash\{V\}$ 表示 $\Pi$
除去 $V$本身。
作用在$V$的子集$C$上的函数$f$
表示$C$和 $C$的补集 $V\backslash C$ 之间边的权值之和，即
$f(C)=\sum_{i \not\in C, j\in C, (i,j) \in E} w_{ij}$。
而作用在$V$的分割$\P$上的函数 $f[\P]$ 则是$f$分别作用$\P$中每一个元素之和。
即 $f[\P]=\sum_{C \in \P} f(C)$。有了如上的符号解释，
式 \eqref{eq:IP} 即不难理解，它表示各分割之间所有边的和对分割总数的平均值。

式 \eqref{eq:IP} 实际给出了一个社团发现的标准，对于扁平化的社团而言，
最小值$\lambda_1$ 对应的社团为
分割$\P^*$的子集。

下面介绍如何求解 $\lambda_1$ 及其对应的分割$\P^*$。实际上，
求解 \eqref{eq:IP} 等价于求解下面的组合优化问题 \cite{mac}
\begin{align}\label{eq:hlambda}
  h(\lambda) &= \min_{\P \in \Pi'} f[\P] - \abs{\P} \lambda 
  \end{align}
其中 $\lambda$ 是一个非负实数。
式 \eqref{eq:hlambda} 中解的结构可以由一系列嵌套的分割
来描述，即存在正整数$k$，使得式 \eqref{eq:hlambda} 的解
为：
\begin{equation}\label{eq:PSP_structure}
  h(\lambda) = \begin{cases} h_{\P_0}(\lambda) & 0\leq \lambda < \lambda_1 \\
  h_{\P_i}(\lambda) & \lambda_i \leq \lambda < \lambda_{i+1} \textrm{ 对于 } i = 1, \dots, k-1 \\
  h_{\P_k}(\lambda) & \lambda \geq \lambda_k
  \end{cases}
\end{equation}
在式 \eqref{eq:PSP_structure} 中，$\lambda_1, \dots, \lambda_{k-1},
\lambda_k$ 是递增的数列，而 $h_{\P}(\lambda)=f[\P]-|\P|\lambda$。
对于分割 $\P_0, \dots, \P_k$ 来说，
它们也有一个序关系。我们称分割 $P$ 是 $Q$ 的一个细分，
如果 $\forall C \in P, \exists C' \in Q, s.t.\, C\subset C'$，
并记为 $P \preceq Q$。基于细分的此种定义，我们有
$\P_k \preceq \dots \preceq \P_1 \preceq \P_0$。
其中 $\P_0=\{V\}$ 而 $\P_k=\cup_{i=1}^n \{ i\}$。

在得到 $h(\lambda)$ 的表达式后，
我们指出 式 \eqref{eq:IP} 和
\eqref{eq:PSP_structure} 中的$\lambda_1$ 取值
相同，因此我们用了相同的符号。而 $\P^*=\P_1$。
从而式 \eqref{eq:IP}的解可以从式 \eqref{eq:PSP_structure}
的解中获得。

下面举一个简单的例子来解释上面的结论。
\begin{example}\label{ex:psp}
考虑一个带权的有向图$G(V,E)$，其结构
如图 \ref{fig:example_directed} 所示。
试求解该图的主分割序列。
\end{example}
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
  \includegraphics[width=\textwidth]{example_directed.pdf}
  \caption{一个有三个节点的带权有向图}
  \label{fig:example_directed}
  \end{subfigure}~
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=\textwidth]{dt.pdf}
    \caption{$h(\lambda)$ 的图像}
    \label{fig:dt}
    \end{subfigure}  
\end{figure}

因为
$G$ 包含的节点只有3个，可以使用枚举法计算对应的 $h(\lambda)$
函数，其函数图像如图 \ref{fig:dt} 所示。其中
$\P_0=\{ \{1,2,3\} \}, \P_1 = \{\{1,3\}, \{2\} \},
\P_2 =\{\{1\},\{2\},\{3\} \}$。
从图 \ref{fig:dt} 中可以看到，在这个简单的例子中
$h(\lambda)$
是一个分段线性函数。一般的图$G$对应的
$h(\lambda)$ 也是如此。

因此，基于图分割的方法，\ref{fig:example_directed}
中的图将被 划分成 $\P^*=\P_1$，即节点1和3被分成一类，
而节点2单独分成1类，这符合我们的直觉。

\newacronym{acr:psp}{PSP}{Principal Sequence of Partition}
\newglossaryentry{psp}{name=主分割序列, description={Principal Sequence of Partition}}
对于一般的图，其节点数目可能很多，此时用枚举法计算
$h(\lambda)$ 变得不可行。\citet{narayanan} 指出，计算
$h(\lambda)$ 有多项式时间的算法。因该算法用到了\gls{psp}(\gls{acr:psp})的结构，
我们把这一算法称为PSP算法。
由于原文献年代较为久远，作为替代，下面我们参考\citet{mac}论文中对PSP算法
的描述对该算法进行进一步介绍。
%\subsection{主分割序列}    

求解 \eqref{eq:PSP_structure} 只需得到在每一个
分界点处对应的 $\lambda$ 和 分割$\P$ 即可。
考虑到 $\P_k \preceq \dots \preceq \P_1 \preceq \P_0$
，
我们有 $|\P_0| \leq |\P_1| \dots \leq |\P_k|$，
即随着 $\lambda $ 的增大，线性函数斜率的绝对值逐渐增大。
这从图 \ref{fig:dt} 中也可以得到印证。
利用这一特点并基于分治的思想，算法 \ref{alg:psp} 
给出了获得主分割序列的步骤。

\renewcommand{\algorithmicrequire}{\textbf{输入：}\unskip}
\renewcommand{\algorithmicensure}{\textbf{输出：}\unskip}

\begin{algorithm}
  \caption{求解主分割序列的算法 (PSP算法)}
  \label{alg:psp}
  \small
  \begin{algorithmic}[1]
    \REQUIRE 图$G$
    \ENSURE 序列 $L=[\lambda_1, \dots, \lambda_k]$
    和 $\mathcal{Q}=[\P_0, \dots, \P_k]$
    \STATE \textbf{L}  $\leftarrow []$
    \STATE $Q\leftarrow \{V\}, P \leftarrow \{ \{i \} | i \in V\}$
    %\STATE $\mathbf{PSP}= [Q, P]$
    \STATE \texttt{Split}$(Q,P)$
    \STATE 对 $L$ 和 $\mathcal{Q}$
    按从小到大排序 \footnotemark
    \FUNCTION{\texttt{Split}$(Q,P)$}
     \STATE\label{alg:lambda} $\lambda' =
     {1 \over \abs{P} - \abs{Q}} (f(P)-f(Q))$
     \STATE\label{alg:lambda_plus} $h' = {1 \over \abs{P} - \abs{Q}}(\abs{P} f(Q) - \abs{Q} f(P))$
     \STATE\label{alg:lambda_f} $(\tilde{h}, P') = \texttt{DT}(G,\lambda')$
     \IF{$\tilde{h} = h'$}
       \STATE\label{algorithme:terminer} 将 $\lambda'$ 插入 $\mathbf{L}$
     \ELSE
       \STATE 将 $P'$ 插入 $\mathcal{Q}$
       \STATE\label{algorithme:gauche} \texttt{Split}$(Q, P')$
       \STATE\label{algorithme:droit} \texttt{Split}$(P',P)$
     \ENDIF
    \ENDFUNCTION
  \end{algorithmic}
\end{algorithm}
\footnotetext{$\mathcal{Q}$
中按集合的大小排序}

在算法 \ref{alg:psp} 第 \ref{alg:lambda} 、 \ref{alg:lambda_plus} 行，
求解 $(\lambda', h')$ 相当于在二维 $(\lambda, h)$
平面上计算直线
$h = f[P] - |P| \lambda $
和 $h = f[Q] - |Q| \lambda $ 的交点。假设 $P, Q$ 对应的
分界点分别是 $\lambda_P, \lambda_Q$（约定 $\{V\}$ 对应 0，
$\{\{i\}|i\in V\}$ 对应 $+\infty$），那么根据几何直观
$\lambda_Q \leq \lambda' \leq \lambda_P$。
紧接着， \ref{algorithme:gauche} 行的调用对应在
$[\lambda_Q, \lambda']$
区间范围内寻找剩余的分界点，而
 \ref{algorithme:droit} 行的调用对应在
$[\lambda', \lambda_P]$
区间范围内寻找剩余的分界点。最后，
 \ref{algorithme:terminer} 行表明
$[\lambda_Q, \lambda_P]$ 内没有分界点，无需再调用
\texttt{Split} 函数。

\newglossaryentry{dt}{name=迪尔沃思截断, description={Dilworth Truncation}}
在算法 \ref{alg:psp} 第 \ref{alg:lambda_f} 行，
函数 \texttt{DT}(\glsdesc{dt})被用来求解式\eqref{eq:hlambda} 获得最优值
$\tilde{h}$ 和最优值对应的分割 $P'$。
%该函数 \texttt{DT}
%
%是用于求解式\eqref{eq:hlambda}的算法，它实际上是一种贪心算法，
%由算法 \ref{alg:dt}  给出。
% \begin{algorithm}
%   \caption{迪尔沃思截断算法}\label{alg:dt}
%   \begin{algorithmic}[1]
%   \REQUIRE 图 $G$ 和 $\lambda$
%   \ENSURE 分割 $\P$ 和 $h$
%   \STATE
%   $V^0 = \emptyset, x $ 是 $n$ 长的向量\footnotemark,
%   $\mathcal{A} = \{\}$
%   \FOR{$l=1, 2, \dots, n$}
%   \STATE $V^l = \{l\} \cup V^{l-1}$
%   \STATE\label{alg:tight} 计算 $x^* = \displaystyle\min_{ A: l \in A \subseteq V^l} f(A)- x(A)$。
%    $T^l$ 是达到此最小值的集合，并且 $x_l \leftarrow x^* - \lambda$。 
%     \STATE $U^l = T^l \cup [\cup \{A | A \in \mathcal{A}, A \cap T^l \neq \emptyset\}] $
%   \STATE $\mathcal{A} = \{U^l\} \cup \{A | A \in \mathcal{A}, A \cap T^l = \emptyset \}$
%   \ENDFOR
%   \STATE $\P^* = \mathcal{A}, h_{\lambda} = x(V)$
%   \end{algorithmic}
%   \end{algorithm}
% \footnotetext{$n$ 是图$G$中节点的个数}
在该函数
% \ref{alg:dt} 
%涉及到一种新的运算记号$x(A)$，
%这里 $x$ 表示一个向量而 $A$ 是一个集合
%$x(A)$ 定义为 $\sum_{i \in A} x_i$。
%此外，采用\cite{chan2017pin}中的方法，我们可以把 \ref{alg:tight}  行中的最优化
中调用了
%问题转化成
求解有向图上的最大流问题的算法。
% 具体而言，
% 针对 $\displaystyle\min_{ A: l \in A \subseteq V^l} f(A)- x(A)$，
% 我们首先构造一个图$G'$包含节点$V'=\{0, 1, 2, \dots, l\}=\{0\} \cup V^l$。
% 节点 $0$ 是相对新加入的，作为最大流算法的源节点。
% 而$l$是目标节点。图$G'$中有向边的权值有如下定义：
% \begin{equation}\label{eq:wij_prime}
%   w'_{ij} = \begin{cases}
%     \max\{0, -x_{i}\} & \textrm{ if } i = 0 \\
%     \max\{0, -x_{i}\} + w_{il} & \textrm{ else if } j = l \\
%     w_{ij} & \textrm{ otherwise }
%   \end{cases}
% \end{equation}
%
% 按照定义式 \eqref{eq:wij_prime}, 源节点0和目标节点j
% 之间的权值为零，即两节点间没有边相连。
% 基于定义好的图$G'$和$s=0,t=l$，我们求解如下
% 标准的最小割问题：
% \begin{equation}\label{eq:mincut}
%   \beta = \min_{A \subseteq V^l: l\in A }
%   c(V' \backslash A, A)
% \end{equation}
%
% 达到式\eqref{eq:mincut}
% 最小值的集合即是 $T^l$，并且$x^*$和 $\beta$
% 有如下关系式：
% \begin{equation}\label{eq:beta_alpha}
%   \beta = x^* + \sum_{x_v > 0, 1\leq v < l} x_v
% \end{equation}
%
%
%\subsection{参数化的最大流算法}
%最大流算法用于解决网络流中从一点到另一点的最优运输路径的问题。
求解最大流问题
有一类基于前置推送-标签重贴(push-relabel) \cite{Goldberg1988} 的算法
具有良好的效率。如果要解一系列的最大流问题并且这一系列的问题
之间有一定的关联性，重复调用最大流算法显然不是最优的选择。
基于这种考虑，有学者提出了参数化的最大流算法\cite{Gallo1989}用于求解
具有特殊结构的一系列的最大流问题。
%下面对参数化的最大流算法
%进行简单的介绍。
%
% 首先让我们回顾一下最大流问题。
% 考虑一有向图$G(V,E)$。其节点集$V$
% 中包含两个特殊的节点：源节点$s$
% 和目标节点$t$。对每一对有向边
% $(u,v)$存在一个非负的容量函数
% $c(u,v)$。如果$u,v$之间没有有向边
% 定义$c(u,v)=0$。
% 最大流问题是求解流量函数
% $f(u,v)$，使得$\sum_{v\in V} f(v,t)$
% 最大。流量函数需要满足如下约束
% \begin{align}
%   f(u, v) \leq c(u, v) \textrm{ for } (u, v) \in V \times V \\
%   f(u, v) = -f(v, u) \textrm{ for } (u, v) \in V \times V \\
%   \sum_{v \in V} f(u,v) = 0 \textrm{ for } v\in V\backslash\{s, t\}
% \end{align}
% 最大流和形如式\eqref{eq:mincut}所示的最小割问题
% 互为对偶问题。因此求解最大流问题的算法也可以
% 用于求解最小割问题。
%
% 参数化的最大流算法考虑的问题是在最大流问题的
% 基础上，假设$c(s,v)$ 
% 是关于$\lambda$的增函数
% 且$c(v, t)$是$\lambda$的减函数，
% 分别记为
% $c_{\lambda}(s,v)$ 和 $c_{\lambda}(v, t)$。
% 对于一系列的$\lambda_1 < \dots < \lambda_k$，
% 我们可以得到 $k$ 个最大流问题。
参数化的最大流算法
基于前置推送-标签重贴算法，通过保留
在$\lambda_{i-1}$上的计算状态并用于下一步的初始化，
以及适当地使用并行化的技术，使得计算$k$ 个最大流问题
的时间复杂度和只计算一个最大流问题近似相等。
\citet{kolmogorov} 基于参数化最大流，将求解式\eqref{eq:hlambda}
转化为求解$n$次参数最大流问题。该并行算法宣称可达到 $O(n^4)$ 的时间复杂度。


\section{随机块模型}\label{sec:sbm}
随机块模型是我们研究社团发现问题主要使用的概率统计模型，我们将在
本小节对其进行简要介绍。与之相关的，我们将介绍随机块模型的精确恢复问题，从而引出误差率的概念；
此外，我们将对随机块伊辛模型这一工作进行简要介绍，该工作构成了我们第 \ref{chap:sibm} 章研究的基础。
最后，我们还介绍了与我们研究有关的两个社团发现算法。

\subsection{随机块模型及其精确恢复问题}\label{sec:exact_recovery}
\newglossaryentry{sbm}{name=随机块模型, description={Stochastic Block Model}}
\newacronym{acr:sbm}{SBM}{Stochastic Block Model}
\gls{sbm}（\gls{acr:sbm}）是社团发现问题中最常用的统计模型之一
\cite{holland1983stochastic, abbe2017community}，
本节介绍我们研究的一类特殊的随机块模型，
%它提供了一个基准人工数据集来评估不同的社团检测算法
%并启发了许多社团检测任务算法的设计 \cite{fortunato2010community}。
%
为此我们首先定义一些常用的符号。随机图仍用符号$G$表示，
其节点集 $V=\{1,\dots, n\}$ 简写成 \gls{not:n_bracket}。
\newglossaryentry{not:n_bracket}
{
  type=notation,
  name={\ensuremath{[n]}},
  description={表示集合$\{1,\dots, n\}$}
}
每个节点的标签为 $X_i$，
\newglossaryentry{not:W}
{
  type=notation,
  name={\ensuremath{W}},
  description={循环群，即$\{1, \omega, \dots, \omega^{k-1}\}$，$k=2$时$\omega=-1$}
}
从 \gls{not:W} $= \{1, \omega, \dots, \omega^{k-1}\}$中取值。
为方便后续讨论，这里我们给$W$赋予群的结构，让它成为一个阶为$k$的循环群，
即$\omega^k=1$，比如复平面上的单位根即有这样的结构。
\newglossaryentry{not:Wn}
{
  type=notation,
  name={\ensuremath{W^n}},
  description={$W$的$n$次笛卡尔积}
}
\gls{not:Wn} 表示 \glsdesc{not:Wn}。 

\newacronym{acr:ssbm}{SSBM}{Symetric Stochastic Block Model}
下面给出有$k$个社团的对称的随机块模型
(\gls{acr:ssbm})
的定义： 
	\begin{definition}[有$k$ 个社团的 SSBM]\label{def:SSBM}
	令 $0\leq q<p\leq 1$, $V=[n]$ 且
  $X=(X_1,\dots,X_n)\in W^n$。 对任意的 $u\in W$，
  $X$ 满足约束
  $|\{v \in [n] : X_v = u\}| = \frac{n}{k}$。
	如果下面两个条件满足，
  则称随机图 $G$ 是通过 $\SSBM(n,k,p,q)$ 模型产生的。 
	\begin{enumerate}
	\item 若 $X_i=X_j$， $G$ 在 节点 $i$ 和节点 $j$之间存在边的概率是 $p$； 
 若 $X_i \neq X_j$，  在 节点 $i$ 和节点 $j$之间存在边的概率是 $q$。
	\item 每条边的存在与否相互独立。
	\end{enumerate}
\end{definition}

在定义\ref{def:SSBM}中，注意到 $p>q$，
说明同一社团之间的节点有边相连的概率更大，而
属于不同社团的节点之间有边相连的概率较小。

\newglossaryentry{Bernoulli_distribution}{name=伯努利分布, description={Bernoulli distribution}}
为进一步解释随机块模型，
我们定义随机变量 $Z_{ij}:=\mathbf{1}[\{i,j\} \in E(G)]$，
它是表示节点$i$和节点$j$之间是否存在边的指示函数。
给定节点的标签向量 $X$，$Z_{ij}$ 服从\gls{Bernoulli_distribution}，
\newglossaryentry{not:expectation}
{
  type=notation,
  name={$\E[\cdot]$},
  description={数学期望}
}
其\glsdesc{not:expectation}值为
\begin{equation}
\E[Z_{ij}] =
\begin{cases}
p & \textrm{ if } X_i = X_j \\ 
q & \textrm{ if }  X_i \neq X_j
\end{cases}
\end{equation}

则 有 $n$ 个节点的随机图 $G$ 
被 
随机变量 $Z:=\{Z_{ij}, 1\leq i<j\leq n\}$ 完全确定。
这里，每一个 $Z_{ij}$ 是相互独立的。
$Z$ 的分布函数可以写为：
\begin{align}\label{eq:mle_sibm}
P_G(G)&:=P_G(Z = z| X=x) \\
&= p^{\sum_{x_i = x_j}
z_{ij}}q^{\sum_{x_i \neq x_j} z_{ij}} 
\cdot (1-p)^{\sum_{x_i = x_j} (1-z_{ij})}
(1-q)^{\sum_{x_i \neq x_j} (1-z_{ij})}
\label{eq:GmL}
\end{align}
\newglossaryentry{mle}
{name=最大似然估计,
description={Maximum likelihood estimation}}
若 参数 $p, q$ 已知，
通过求
式\eqref{eq:GmL} 的最大值
我们可以获得节点标签的估计量$\hat{X}$，此即
利用\gls{mle}做社团发现。
%\footnote{Maximum Likelihood, 缩写为 ML}。
\newglossaryentry{not:cGn}
{
  type=notation,
  name={$\cG_n$},
  description={所有包含$n$个节点的图的集合}
}

这里顺便介绍一个后文中会用到的符号 \gls{not:cGn}，它表示
\glsdesc{not:cGn}。
由概率分布的归一化的性质可得，
$P_G(\cG_n) = \sum_{G\in \cG_n}P_G(G)=1$。
\newacronym{acr:ml}{ML}{Maximum Likelihood}

不加说明的情况下，最大似然估计算法 是无约束的。但在
定义\ref{def:SSBM}的假设下，
还有一个额外的约束，即 $X$ 中每一类的标签数量是严格相等的。
在这个约束下最大化式\eqref{eq:GmL} 我们可以得到一个等价
的优化问题——$k$类的最小割问题。
其形式为：
\begin{equation}\label{eq:minimum_k_cut}
  \min_{x\in W^n} \sum_{ \{i,j\} \in E} (1-\delta(x_i, x_j))
\end{equation}
这里，示性函数 $\delta(x,y)$ 定义为：
\glsdesc{not:deltaxy}。
\newglossaryentry{not:deltaxy}
{
  type=notation,
  name={$\delta(x,y)$},
  description={当 $x=y$ 时，$\delta(x,y) = 1$； 当 $x\neq y$，$\delta(x,y)=0$}
}

\newglossaryentry{planted}{name=植入性分割模型, description={Planted partition model}}
当$k=2$时，式\eqref{eq:minimum_k_cut} 即为\gls{planted}，
是图论中经典的NP难的二分问题之一。


随机块模型的精确恢复是指某社团发现算法用在随机块模型上可以将
每个节点的类别都正确地分出来。因为随机块模型具有随机性，
这里正确的分出来是指当图的节点数目趋于无穷大时，正确概率收敛到1。
为给出精确恢复严格的数学定义，需要引入如下置换的符号。

\newglossaryentry{not:dist}
{
  type=notation,
  name={$\Dist(\sigma, \sigma')$},
  description={两个$W^n$中的向量的距离，即$|\{i\in[n]:\sigma_i\neq \sigma'_i\}|$}
}
我们用 $S_k$ 表示 $W$  上所有的置换函数， 
$f$ 是 定义在 $W$ 上的置换函数
并且可以通过逐元素作用的方式
将其定义域扩展到 $W^n$ 上。
对于任意 $\sigma \in W^n$，
定义集合 $S_k(\sigma):=\{f(\sigma)| f\in S_k\}$。
此外，我们定义两个$W^n$中的向量的
距离为\gls{not:dist}$=|\{i\in[n]:\sigma_i\neq \sigma'_i\}|$，
其中 $\sigma,\sigma'\in W^n
$。
\begin{example}
当 $n=2$ 且 $k=2$ 时，
取$\sigma=(1, \omega) \in W^2$。
$\omega$满足
$\omega^0 = 1$ 且 $\omega \cdot \omega = \omega^2 = 1$。
令 $f$ 是一个$W$上映射，定义为
$f(1) = \omega$ 且 $f(\omega)=1$，
则 $f \in S_2$ 且 $f(\sigma) = (\omega, 1)$。
另外我们有 $\Dist(\sigma, f(\sigma)) = 2$，
$S_k(\sigma) = \{\sigma, f(\sigma)\}$， 且
$S_k^c(\sigma) = W^2 \backslash S_k(\sigma)
=\{(1, 1), (\omega, \omega)\}$。
\end{example}

给定 随机块模型，精确恢复问题的数学定义如下:
\newglossaryentry{exact_recovery}{name=精确恢复, description={Exact Recovery}}
\begin{definition}[SBM 中的\gls{exact_recovery}] \label{def:SSBMR}
给定节点标签 $X$， 并假设随机图 $G$ 从 $\SSBM(n,k,p,q)   $ 模型中采样。
一个社团发现算法$\hat{X}$ 已知 $G$ 去估计$X$，
也被称做$X$ 的估计量。我们称该算法
可精确恢复$X$，如果
\begin{equation}\label{eq:Pa_hat_X}
P_a(\hat{X}):=P(\hat{X} \in S_k(X)) \to 1 \textrm{ 当 }\, n \to \infty
\end{equation}
\end{definition}

在上述定义中，记号 $\hat{X} \in S_k(X)$ 表示
我们只能获得置换意义下相对于真实标签$X$的恢复结果。
这是因为没有一个类别的基准导致的。
这种情况在无监督学习领域比较常见。
式\eqref{eq:Pa_hat_X}中记号 $P_a(\hat{X})$
表示 估计量 $\hat{X}$ 的
准确概率。
\begin{remark}\label{rem:metric_exact_recovery}\,
  \begin{enumerate}
    \item 令 $P_e(\hat{X}) = 1 - P_a(\hat{X})$
    表示错误概率。
    定义\ref{def:SSBMR}也可以写成当$n\to \infty$
    时，
    $P_e(\hat{X}) \to 0$。
  \item  此外，我们要指出的是，对给定的图 $G$，估计量$\hat{X}$可以是确定性的也
  可以是随机的。
  一般而言， $\hat{X} \in S_k(X)$ 发生的概率 应该理解
  对随机图的平均 $\sum_{G \in \cG_n} P_G(G) P_{\hat{X}|G}(\hat{X} \in S_k(X))$。 
  如果是非随机的图，这个概率是 $P_G(\hat{X} \in S_k(X))$。  
  \end{enumerate}
\end{remark}

%在定义\ref{def:SSBMR}中，精确恢复也称强恢复。强恢复是相对于弱恢复
%而言的，前者要求所有节点都要分对，而后者只要求分对节点的比率收敛到1即可。
%关于这两种恢复条件近年来涌现大量相关的研究工作，
%因为我们的工作只涉及强恢复，
基于定义\ref{def:SSBMR}，
下面对一些和强恢复有关的重要结论进行逐一介绍。

设$a>b>0$，针对两个社团（$k=2$）且$p=\A,q=\B$的情形，
Abbe \cite{abbe2015exact} 和 Mossel
\cite{mossel2016} 分别独立地研究发现了最大似然算法
精确恢复误差的变化规律。
因为最大似然算法相当于理论最优的估计量，他们的成果可总结为如下定理：
\begin{theorem}\label{thm:sbm2_phase_transition}
设 $P_e=P(\hat{X} \neq \pm X)$: 当 $n \to \infty$，
对于 $\SSBM(n,2,\A, \B)$ 模型，
有如下两种情形：
	\begin{enumerate}
		\item $\sqrt{a} - \sqrt{b} > \sqrt{2}$时，
    $P_e \to 0$， 存在算法满足精确恢复的要求;
		\item $\sqrt{a} - \sqrt{b} < \sqrt{2}$时，
    $P_e \to 1$，没有算法可实现精确恢复。
	\end{enumerate}
\end{theorem}
定理\ref{thm:sbm2_phase_transition}揭示了
具有2个社团结构的随机块模型在精确恢复度量下的相变规律。
该结论很快被Abbe 推广到$k>2$的情形
\cite{abbe2015community}，由
定理 \ref{thm:sbmk_phase_transition} 给出。
该结果可以视为一般的随机块模型
精确恢复条件的一个特例。

\begin{theorem}\label{thm:sbmk_phase_transition}
  对于 $\SSBM(n,k,\A, \B)$ 模型，当条件
  \begin{equation}\label{eq:abk}
    \sqrt{a} - \sqrt{b} > \sqrt{k}
  \end{equation}   
  满足时，
  精确恢复可实现，
  而当$\sqrt{a} - \sqrt{b} < \sqrt{k}$时，
  没有算法可实现精确恢复。
\end{theorem}
\newglossaryentry{hellinger}{name=海林格距离, description={Hellinger distance}}
\begin{remark}
  Abbe 在文章中指出，定理 \ref{thm:sbmk_phase_transition} 
  给出的结论和\gls{hellinger}(\glsdesc{hellinger})有关。
  % 对于两个$\R^d$中的向量$u,v$，它们的
  % 海林格距离 
  % 的平方定义为
  % \begin{equation}
  %   H^2(u,v) = \frac{1}{2}
  %   \sum_{i=1}^d (\sqrt{u_i} - \sqrt{v_i})^2
  % \end{equation}
  % 若定义向量 $u= (\frac{a}{k}, \frac{b}{k})$,
  % $v =  (\frac{b}{k}, \frac{a}{k})$,。
  % 则
  % $\sqrt{a} - \sqrt{b} > \sqrt{k}$ 的条件
  % 可改写成
  % $H(u, v) > 1$，其中
  % \begin{equation}\label{eq:Hellinger_abuv}
  %   H(u,v)=\frac{\sqrt{a} - \sqrt{b}}{\sqrt{k}}    
  % \end{equation}
  % 上面介绍的是Abbe 引入的切尔诺夫-海林格距离的定义，
  但他在文章中使用的距离度量仅是一般的测度而不是概率测度，
  其解释略显生硬。
\end{remark}

\subsection{随机块-伊辛模型}\label{sec:ising}
\begin{figure}
	\centering
	\begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{Tlarge.pdf}
		\caption{$T>T_c$， 自旋方向随机}
	\end{subfigure}~
	\begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{Tsmall.pdf}
		\caption{$T<T_c$， 自旋方向一致}
	\end{subfigure}
  \caption{磁偶极矩自旋方向随温度变化示意图}\label{fig:ising_two_configurations}
\end{figure}   

在统计物理学中，伊辛模型指的是
可以处于两种自旋状态的磁偶极矩的集合\cite{ising1925beitrag}。
如图 \ref{fig:ising_two_configurations} 所示，
当外界温度$T$
大于临界温度 $T_c$ 时，
各磁偶极矩处于杂乱无章的状态，其总的磁化强度接近0。
但当外界温度$T$
小于临界温度 $T_c$ 时，
各磁偶极矩的状态则变得方向一致。
这里，我们称这种方向一致的现象叫自发磁化。


\begin{figure}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{square-lattice.png}
		\caption{正方形网格}\label{fig:square_lattice}
	\end{subfigure}~
	\begin{subfigure}{0.53\textwidth}
		\includegraphics[width=\textwidth]{monte-carlo-ising-6.png}
		\caption{$M$随温度变化情况}\label{fig:square_lattice_b}
	\end{subfigure}
  \caption{正方形网络上的伊辛模型及其磁化强度的变化}
\end{figure}

下面我们介绍伊辛模型中一些常用的概念，
如图 \ref{fig:square_lattice} 所示，
在一个正方形网格中的格点上分布着磁偶极矩，一共有$N$个。
$\sigma_i \in \{ \pm 1\} $ 表示第$i$个磁偶极矩的状态，
总的磁化强度为 $M = \frac{1}{N} \sum_{i=1}^N \sigma_i$。
理论研究时先考虑有限的正方形网格， 再取 $N\to \infty$。
由于磁偶极矩之间的磁力作用和受外场的影响，存在着磁化强度的相变现象，即
\begin{enumerate}
		\item $T< T_c, M>0$， 发生自发磁化。
		\item $T> T_c, M=0$， 无自发磁化。
\end{enumerate}

\newglossaryentry{phase_trans}{name=相变, description={Phase transition}}
图 \ref{fig:square_lattice_b} 中给出了自发磁化的仿真实验结果，其中临界温度用
红竖线标出。从图中可见在临界温度处磁化强度突然降为零，也即
磁偶极矩的自旋方向由完全一致变成两种状态各占一半。我们称临界温度处伊辛模型
的这种变化为相变（\glsdesc{phase_trans}）现象，
我们在第 \ref{chap:sibm} 章也将展示社团发现的精确恢复问题
中也有类似图 \ref{fig:square_lattice_b} 的相变现象。

对于自发磁化现象可以通过统计力学中的正则系踪（Canonical ensemble）
理论进行解释。该理论
假设粒子的分布为
\begin{equation}\label{eq:canonical_ensemble}
P(\sigma = \bar{\sigma}) = \frac{1}{Z} \exp(-\beta H(\bar{\sigma}))
\end{equation}
其中$\sigma \in \{\pm 1\}^n$ 表示粒子的状态，
$H$叫做汉密尔顿能量函数，表示系统处于状态$\bar{\sigma}$时的总能量，$\beta$是与温度成反比的参数，
\newglossaryentry{inverse_temp}{name=逆温度, description={Thermodynamic beta}}
也叫逆温度（\glsdesc{inverse_temp}），$Z$是配分函数，是概率分布的
归一化常数。


伊辛模型除了定义在正方形网格上外，还可以推广到一般的图 $G(V, E)$ 上。
对于没有外场的情形，伊辛模型的能量函数为
\begin{equation}\label{eq:hamiltonian}
	H(\sigma) = -\sum_{\{i,j\} \in E(G)} \sigma_i \cdot \sigma_j
\end{equation}
在微观层面上，每种微观状态出现的概率均正比于 $\exp(-\beta H(\bar{\sigma}))$，
概率越大或能量越低的状态越有可能出现；在宏观层面则表现为
磁化强度$M$的变化。

\begin{example}
  表\ref{tab:particles_3}给出了一个含有3个粒子的系统，其共有8种微观态，
  但根据式\eqref{eq:hamiltonian}， 其在宏观能量的表现了只有$H=-3$或 $H=1$两种组合。
\begin{table}
  \centering
\begin{tabular}{ccc}
		编号 & 微观态 & 宏观态 ($H$) \\
		1 & $\uparrow\uparrow\uparrow$ & -3 \\
		2 & $\uparrow\uparrow\downarrow$ & 1 \\
		3 & $\uparrow\downarrow\uparrow$ & 1 \\
		4 & $\downarrow\uparrow\uparrow$ & 1 \\
		5 & $\uparrow\downarrow\downarrow$ & 1    \\
6 & $\downarrow\uparrow\downarrow$ & 1 \\
7 & $\downarrow\downarrow\uparrow$ & 1 \\
8 & $\downarrow\downarrow\downarrow$ & -3 \\
\end{tabular}
\caption{一个含有3个粒子的系统的微观状态与宏观能量}
\label{tab:particles_3}
\end{table}
\end{example}

\newglossaryentry{not:all_one_vector}
{
  type=notation,
  name={\ensuremath{\mathbf{1}_n}},
  description={长度为 $n$ 的全1的向量}
}

将伊辛模型视为网络中一种刻划节点状态的概率模型，
有学者\cite{ye2020exact}
指出，式\eqref{eq:hamiltonian} 所描述的
伊辛模型不存在相变现象，因为式\eqref{eq:canonical_ensemble}
给出的分布聚集在 $\sigma=\pm \mathbf{1}_n$ 附近，
这里 \gls{not:all_one_vector} 表示\glsdesc{not:all_one_vector}。
为此，可以采用下述修正模型，将没有边相连的粒子之间的
作用力考虑进来。
\begin{equation}\label{eq:ising_modified}
  H(\bar{\sigma}) = \gamma \frac{\log n}{n} \sum_{\{i,j\}\not\in E(G)}
  \bar{\sigma}_i  \bar{\sigma}_j
	- \sum_{\{i,j\}\in E(G)}
  \bar{\sigma}_i  \bar{\sigma}_j
\end{equation}
这里引入了一个新的参数 $\gamma > 0$，表示反作用力的系数。
\eqref{eq:ising_modified}
相比于\eqref{eq:hamiltonian}
多了反作用力的项，在物理中类似自旋玻璃的模型\cite{lenka2016physics}。
因为该工作研究的是
由随机块模型 $\SSBM(n,2,p,q)$产生的图上定义的伊辛模型，
式\eqref{eq:ising_modified} 出现的系数
$\frac{\log n}{n}$
主要是为了与其研究场景 $p,q=O(\frac{\log n}{n})$
相对应。

\newglossaryentry{metropolis}{name=梅特罗波利斯算法, description={Metropolis algorithm}}

梅特罗波利斯（Metropolis）算法\cite{metropolis1953equation}
被用于产生伊辛模型的样本。此外，该算法的变体“模拟退火”
\cite{pincus1970monte} 也是优化领域常用的随机算法。
其算法从系统一个随机的状态出发每次改变其中一个粒子
的状态并按一定的概率接受改变或保持原来的状态。
给定形如式\eqref{eq:ising_modified}
的能量函数，算法\ref{alg:Metropolis}
给出了\gls{metropolis}的步骤。

\begin{algorithm}
  \caption{梅特罗波利斯算法}\label{alg:Metropolis}
  \begin{algorithmic}[1]
    \STATE 随机初始化 $\bar{\sigma}$
    \STATE 随机选一个粒子进行状态翻转，新的系统状态记为 $\bar{\sigma}'$ 
    \STATE 计算能量差 $\Delta H= H(\bar{\sigma}') - H(\bar{\sigma})$
    \IF{$\Delta H < 0$}
    \STATE $\bar{\sigma} \leftarrow \bar{\sigma}'$
    \ELSE
    \STATE 以概率 $\exp(-\beta \Delta H)$ 
    使得 $\bar{\sigma} \leftarrow \bar{\sigma}'$，
    否则保持原状。 
    \ENDIF
    \STATE 重复步骤 2-8 直到收敛。
\end{algorithmic}  
\end{algorithm}

以上讨论的伊辛模型中粒子均只有$\{\pm 1\}$两个状态。
实际上每个粒子的状态数可以扩展到多个，即 玻茨模型\cite{potts1952some}。
类似式\eqref{eq:hamiltonian}，标准玻茨模型的汉密尔顿能量公式为：
\begin{equation}\label{eq:Hsigma_multiple_state}
  H(\sigma) = -\sum_{(i,j) \in E(G)}\delta(\sigma_i, \sigma_j)
\end{equation}
\begin{remark}\label{rem:equivalence_H_energy}
需要注意的是，在状态数$k=2$的情形
式\eqref{eq:Hsigma_multiple_state} 
与式\eqref{eq:hamiltonian}
并不严格等价。
从关系式$\delta(\sigma_i, \sigma_j) = \frac{\sigma_i \sigma_j + 1}{2}$
可以看出，它们之间差了一个伸缩变换。
\end{remark}

\newglossaryentry{sibm}{name=随机块-伊辛模型, description={Stochastic Ising Block Model}}
\newacronym{acr:sibm}{SIBM}{Stochastic Ising Block Model}
将 \ref{sec:exact_recovery} 小节 介绍的随机块模型与 本小节介绍的 伊辛模型
复合起来可得到\gls{sibm}(\gls{acr:sibm})\cite{ye2020exact}。
该模型首先由 随机块模型生成随机图，
再由随机图上定义的 伊辛模型多次生成节点的状态。
SIBM 模型的恢复任务是由多次生成的节点状态估计节点的真实状态。
该复合模型可用于模拟两党制国家政治选举前的多次民意调查，
生成随机图依据的节点真实状态即为选民
的政治倾向。下面对SIBM的研究结果进行简要介绍。

在\citet{ye2020exact} 的研究工作中，由 $\SSBM(n,2,\A, \B)$
生成随机图，采用 \eqref{eq:ising_modified} 定义的能量函数
独立生成 伊辛模型的样本 $m$ 次，主要研究了样本复杂度$m$
的相变现象以及当$m=1$时参数$\beta$的相变现象。
这里的相变现象是针对SBM模型的精确恢复问题而言的，
即对于某一参数存在一个精确恢复能否实现的临界值点。

\citet{ye2020exact} 的工作中关注的重点是对采样数量$m$的相变现象的讨论，
而我们将在其工作的基础上，将伊辛模型拓展为节点有多个状态的玻茨模型，
重点研究$m=1$时参数$\beta$的相变现象以及相关的社团发现算法。

\citet{ye2020exact} 在其工作的技术证明中，用到了马尔可夫不等式和切尔诺夫不等式这两个概率论中常用的聚集不等式。
由于我们在后面章节的技术证明中也要用到这两个不等式，故在本小节最后列出它们，方便后面引用。

\newglossaryentry{markov_ieq}{name=马尔可夫不等式, description={Markov Inequality}}
\begin{lemma}[\gls{markov_ieq}] 
  设 $X$ 是非负型随机变量，则对于非负实数$a$有：
  \begin{equation}
    P(X\geq a) \leq {1 \over a} \E[X]   
  \end{equation}
   若$X$ 不一定非负，$a$ 是任意实数，可先取$e$指数再运用马尔可夫不等式，
   即有
   \begin{equation}\label{eq:chernoff_bound}
    P(X\geq a) = P(e^{tX} \geq e^{ta}) \leq { 1 \over \exp(ta)} \E[\exp(tX)]
   \end{equation}
   对任意的$t>0$成立。
   \newglossaryentry{chernoff}{name=切尔诺夫不等式, description={Chernoff Inequality}}

   式\eqref{eq:chernoff_bound}也被称为\gls{chernoff}。
\end{lemma}
\newglossaryentry{chebyshev_ieq}{name=切比雪夫不等式, 
description={Chebyshev's Inequality}}
\begin{lemma}[\gls{chebyshev_ieq}]
  \newglossaryentry{not:variance}
{
  type=notation,
  name={$\Var[\cdot]$},
  description={方差}
}
  对于随机变量 $X$， 正实数$a$，我们有
  \begin{equation}
    P(\abs{X-\E[X]} \geq a) \leq {1 \over a^2} \Var[X]
  \end{equation}
  这里，$\Var[X]$表示随机变量$X$的\glsdesc{not:variance}。
\end{lemma}

% to do: add large deviation theory and hypothesis testing


\subsection{最大模块度算法}

最后，我们再简单介绍一下社团发现领域常用的最大模块度算法
\cite{newman2006modularity}。
\newglossaryentry{modularity}{name=模块度, description={Modularity}}
\gls{modularity}(\glsdesc{modularity})
是对网络中社团结构强弱程度的一种度量，既能作为评价
不同社团发现算法的性能指标，也能
通过近似求解其最大值的思路启发算法设计。
模板度越高，则社团内部连接越紧密，
而社团之间连接越稀疏。

给定$n$长的向量$x$，模块度的数学定义为：
\begin{align}\label{eq:Q}
  Q(x) &= \frac{1}{2 |E|} \sum_{ij} (A_{ij} - \frac{d_i d_j}{2 |E|}) \delta(x_i, x_j)
\end{align}
这里 $A$ 表示图的邻接矩阵，$|E|$表示边的数量，
而$d_i$表示第
\newglossaryentry{degree}{name=度, description={Degree}}
$i$个节点的\gls{degree}(\glsdesc{degree})，$x_i$ 表示第$i$
个节点所属的社团。

贪心法是用于近似求解式\eqref{eq:Q}
最大值的常用算法，其流程类似于系统聚类法，
即从每个节点各自属于一类出发，通过中间指标最大的方法每次聚合两类，
使得 $Q$ 不断增大。该思路可经过优化适用于求解大规模网络中
的社团发现问题 \cite{clauset2004finding}。

\subsection{基于半正定规划的社团发现算法}
\newacronym{acr:sdp}{SDP}{Semi-definite programming}
\newglossaryentry{sdp}{name=半正定规划, description={Semi-definite programming}}

针对随机块模型的社团发现
可以先将最大似然方法获得的目标函数
写成整数优化问题的形式，
再转化成 \gls{sdp} (\gls{acr:sdp}) 的优化问题进行求解。

在 式\eqref{eq:minimum_k_cut} 中
取 $\delta(y_i, y_j) = \frac{1+y_iy_j}{2}$，
于是可得到如下
的优化问题：
\begin{align}
  \max_x\, &  x^T A x \notag \\
  \textrm{s.t. } & x_{i} = 1, i \in [n] \notag \\
  & \mathbf{1}^T x = 0
\end{align}
其中 $A$ 表示图的邻接矩阵。
$ x^T A x $ 可以看成是 $\langle xx^T, A \rangle $，即矩阵
\newglossaryentry{not:matrix_inner_product}
{
  type=notation,
  name={$\langle A, B \rangle $},
  description={矩阵$A$ 和 $B$ 的内积}
}
$xx^T$ 与 $A$ 的内积。令$Y=xx^T$，则$Y$是一个
半正定的矩阵，若对$Y$秩为1的条件进行松弛，则
得到如下半正定规划问题：
\begin{align}\label{eq:opt_A_Y_1}
  \max_Y\, & \langle A, Y \rangle  \notag \\
  \textrm{s.t. } & Y_{ii} = 1, i \in [n] \notag \\
  & \langle Y, J_n \rangle  = 0
\end{align}
\newglossaryentry{not:matrix_all_one}
{
  type=notation,
  name={$J_n$},
  description={元素全为1的$n\times n$的矩阵}
}
其中 $J_n=\mathbf{1}_n\mathbf{1}_n^T$
是\glsdesc{not:matrix_all_one}。
令 $B=2A + I_n - J_n$，
\newglossaryentry{not:identity_matrix}
{
  type=notation,
  name={$I_n$},
  description={$n\times n$的单位矩阵}
}
$I_n$ 是 \glsdesc{not:identity_matrix}，
则
优化的目标函数还可写成 $ \langle B, Y \rangle $，
其中 $B$的定义为
\begin{equation}\label{eq:def_B_sdp}
    B_{ij} = \begin{cases}
        1, & \text{若 $i$ 与 $j$ 有边相连}, \\
        -1,& \text{否则}
    \end{cases}
\end{equation}  

以上方法可针对一般的图$G$进行求解，
但对于$\SSBM(n,2, \A, \B)$，
Hajek 等学者 \cite{hajek2016achieving} 得到了SDP在随机块模型中检测社团的
理论保证。
\begin{theorem}
  设 $\hat{Y}$ 是
  优化问题 \eqref{eq:opt_A_Y_1} 的解，$\sigma$ 是节点的真实标签向量。
  若 $\sqrt{a} - \sqrt{b}
  > \sqrt{2}$，
  则
  $\lim_{n\to\infty} P(\hat{Y}=\sigma\sigma^T)=1$。
\end{theorem}
\section{信息论度量}
由于本文要用到多种信息论度量研究
社团发现问题，下面依次对多变量互信息、速率函数、雷尼散度等相关知识进行简要介绍。
另外，由于我们在用多变量互信息研究社团发现算法过程中还用到了局部信息几何相关的知识，
本节也对局部信息几何做一个简要的介绍。
\subsection{多变量互信息}\label{sec:info_clustering}
在信息论领域，互信息提供了两个随机变量之间的
距离度量。使用 KL 散度的记号，随机变量$X$
和 $Y$ 之间的互信息定义为：
\begin{equation}\label{eq:mutual_info}
  I(X;Y) = D(P_{XY} ||P_XP_Y)
\end{equation}
\newglossaryentry{not:kl_dev}
{
  type=notation,
  name={$D(P_X||P_Y)$},
  description={分布$P_X$ 和 $P_Y$之间的 KL 散度}
}
\newglossaryentry{mmi}{name=多变量互信息, description={Multivariate mutual information}}
\newacronym{acr:mmi}{MMI}{Multivariate mutual information}
其中 $P_X, P_Y$表示边缘分布，$D(P_X||P_Y)$
表示\glsdesc{not:kl_dev}。
而$P_{XY}$
表示两个随机变量的联合分布。有多种方法可将互信息
的概念推广到两个及两个以上的随机变量间，
其中\citet{ska} 提出了一种推广 具有多种好的性质，
且涵盖了图结构的情形，我们把他的推广叫做\gls{mmi} (\gls{acr:mmi})。
这里我们需要指出，MMI 并不是多个随机变量之间信息量的唯一度量方法，
比如有学者基于HGR最大相关提出了广义最大相关(generalized maximal correlation)
也能实现类似的目的 \cite{huang2020information}。

下面我们对MMI做简要的介绍。令$V$是随机变量
的序号集合，$C$是 $V$的子集，$Z_C=\{Z_i | i \in C\}$
表示随机变量序号在 $C$ 中的随机变量 $Z_i$ 的集合。
$P_{Z_C}$表示$Z_C$的联合分布。
$\P, \Pi'$等符号在 \ref{sec:graph_strength} 节定义。
则MMI度量的定义为：
\begin{align}
  I(Z_V) &:= \min_{\P \in \Pi'} I_{\P}(Z_V),
  \textrm{ 其中 } \label{eq:IZV}\\  
  I_{\P}(Z_V) &:= \frac{1}{|\P| - 1}D(P_{Z_V} || \prod_{C\in \P} P_{Z_C}) \label{eq:IPZV}
\end{align}
在上面的定义中，如果$Z_V$彼此独立，那么$I(Z_V)=0$，
反之亦然。

式\eqref{eq:IPZV}给出了特定分割下的多变量互信息，
也可根据下式把KL散度改写成熵的形式。
\begin{equation}\label{eq:entropy_expression}
  D(P_{Z_V} || \prod_{C\in \P} P_{Z_C}) 
  = \sum_{C \in \P}
  H(Z_C) - H(Z_V)
\end{equation}
但对于一般的分布，式\eqref{eq:IZV}
难于计算。\citet{ska} 又提出了一种特殊的PIN模型
(pairewise independent network)\cite{chan2017pin}，
使得多变量互信息可以用 \ref{sec:community_detection} 节
介绍的PSP算法进行计算。
这里略去PIN模型的数学定义，而是通过下面的例子
阐发它和图结构的联系。
\begin{example}\label{ex:mmi}
  令$V=\{1,2,3\}$，$Z_1=(X_a, X_c)$，
  $Z_2=(X_a,X_b)$ 且 $Z_3=(X_b,X_c)$。
  $X_a,X_b,X_c$是独立的随机变量且它们的熵分别为：
  $H(X_a)=H(X_b)=1, H(X_c)=5$。
  利用联合熵的公式不难计算出$H(Z_1)=H(X_a)+H(X_c)$以及
  $H(Z_1, Z_2) = H(X_a) + H(X_b) + H(X_c)$等。
  若 $\P=\{\{1,2\}, \{3\}\}$，则根据
  \eqref{eq:entropy_expression}式，
  $D(P_{Z_V}||P_{Z_1,Z_2}P_{Z_3}) = H(X_a)
  +H(X_b)$。我们可以将上面的计算用图 \ref{fig:example_directed} 
  进行对应，即把$Z_i$对应到第$i$的节点，
  $Z_i$与$Z_j$之间的互信息对应到第$i$和第$j$个节点之间
  边的权值，而KL散度即为不同的类别之间边的权值之和。
\end{example}
例 \ref{ex:mmi} 中虽然只有三个节点，
但对于一般的情形，PIN模型也是和一个有向图有
着类似的对应关系。因此，在PIN模型下多变量互信息可以用
PSP算法进行计算。

基于多变量互信息可以对随机变量进行聚类，聚类簇
定义为
\begin{align}
  C(Z_V) &:= \bigcup_{\gamma \in \R} C_{\gamma}(Z_V)
  \label{eq:czv}\\
  C_{\gamma}(Z_V) &:= \textrm{maximal}
  \{ B \subseteq V \big\vert |B|>1, I(Z_B) > \gamma  \}
\end{align}
理论分析表明，基于定义式\eqref{eq:czv} 可以得到和
式\eqref{eq:PSP_structure}类似的层次聚类结果。
即存在一系列的临界值$\lambda_1 < \dots < \lambda_k$和分割
$\P_k \preceq \dots \preceq \P_0$，使得
$C(Z_V) = \bigcup_{i=1}^{k} C_{\lambda_i}(Z_V)$
且$C_{\lambda_i}(Z_V) = \P_i$。

最后，我们再介绍一个多变量互信息的性质，它在我们后面研究异常值检测问题中
用到。这个性质给出了最大的临界值 $\lambda_k$
的多变量互信息的表达式\cite{agg_ic}：
\begin{equation}\label{eq:largest_threshold}
\lambda_k = \max_{A\subseteq V} I(Z_A)
\end{equation}

\subsection{局部信息几何}\label{sec:local_geometry}
局部信息几何\cite{huang2019universal}
可用于从理论层面上分析机器学习中的特征提取\cite{huang2017information}、人工神经网络
\cite{huang2019information}等问题。
下面对本工作中用到的局部信息几何的内容进行简要的介绍。

KL散度可用于度量两种不同概率分布
之间的距离。当这两种分布比较接近时，
局部信息几何刻画了KL散度的近似表达式。
我们以离散分布为例，给出表示分布之间距离接近的
形式化定义。
\newglossaryentry{pmf}{name=概率质量函数, description={Probability mass function}}
$P_0$ 表示一\gls{pmf} (Probability mass function)，
其定义域为字母集$\mathcal{X}$，
$\epsilon$表示一无穷小量。
\begin{definition}\label{def:eps_neighborhood}
我们称 $P$ 在 $P_0$的 $\epsilon$ 邻域 $N^{(\epsilon)}(P_0)$ 内，如果
\begin{equation}\label{eq:P_neighborhood}
P \in N^{(\epsilon)}(P_0) \iff
\sum_{x \in \mathcal{X}} \frac{(P(x) - P_0(x))^2}{P_0(x)} \leq \epsilon^2
\end{equation}
\end{definition}
式\eqref{eq:P_neighborhood}可以写成 $P(x) = P_0(x) + \epsilon
\sqrt{P_0(x)} \phi(x) + o(\epsilon)$的简化形式，其中 $\phi(x)$
叫做特征函数，它的欧氏范数 $||\phi || $ 不超过1。
对于两个在$P_0$的$\epsilon$邻域内的分布 $P_1, P_2$，我们有
\begin{equation}\label{eq:approx:ig}
D(P_1 || P_2) = \frac{\epsilon^2}{2} ||\phi_1 - \phi_2||^2 + o(\epsilon^2)
\end{equation}
对于两个弱独立的随机变量，也有类似式\eqref{eq:approx:ig}
的结论。
\begin{definition}\label{def:weak_indepedent}
我们称 $X$ 和 $Y$ 是 $\epsilon$-弱独立的，
如果 对于任何 $x \in \mathcal{X}$，
$Y$关于$X$的条件分布
$P_{Y|X}(\cdot |x)$在$Y$的$\epsilon$邻域内。
\end{definition}
在弱独立的假设下，由 $D(P_{XY}||P_XP_Y)$ 所定义的互信息$I(X;Y)$, 
通过对式\eqref{eq:approx:ig}的化简，可以表示为：
\begin{equation}\label{eq:Ixy}
I(X;Y) = \frac{1}{2}\sum_{x\in \mathcal{X}, y\in \mathcal{Y}} B^2(x,y) + o(\epsilon^2)
\textrm{ 其中 }  B(x,y)=\frac{P_{X,Y}(x,y) - P_X(x) P_Y(y)}{\sqrt{P_X(x)P_Y(y)}}
\end{equation}
在式\eqref{eq:Ixy}中， $\sum_{x\in \mathcal{X}, y\in \mathcal{Y}} B^2(x,y)$
一项可以写成 $\norm{B}_F^2$。这里 $B$表示$\mathcal{X} \times \mathcal{Y}$的矩阵，
\newglossaryentry{not:norm_f}
{
  type=notation,
  name={$||\cdot||_F$},
  description={矩阵的Frobenius范数}
}
$F$表示\glsdesc{not:norm_f}。
式 \eqref{eq:Ixy} 表明，当$X$和$Y$弱独立时，
$I(X;Y)$可用$\frac{1}{2} \norm{B}_F^2$
近似，其阶数为 $O(\epsilon^2)$。

\begin{example}\label{ex:Pweak_1}
考虑 $P_0=(X_0,Y_0)$是一个两维的随机向量，对于$x,y \in \{0,1\}$，
$P_0(x,y)=\frac{1}{4}$。
不难看出$X_0,Y_0$
是独立同分布的伯努利随机变量。
定义 $P(x,y)=\frac{1}{4}(1+\epsilon (-1)^{x+y})$，
则 $P\in N^{(\epsilon)}(P_0)$，而
$\phi(x,y) = \frac{(-1)^{x+y}}{2}$
且有 $||\phi||=1$。
利用 $\log(1+x) = x - \frac{1}{2}x^2 + o(x^2)$
不难得到 $D(P_0||P)=\frac{\epsilon^2}{2}
+o(\epsilon^2)$，从而印证了
式\eqref{eq:approx:ig}的结论。
此外，根据定义 \ref{def:weak_indepedent} 
我们可以验证$X$与$Y$是弱独立的。
这里，$X\times Y$ 服从分布$\frac{1}{4}(1+\epsilon (-1)^{x+y})$。
由式\eqref{eq:Ixy}可计算出
$I(X;Y)=\frac{1}{2}\epsilon^2+o(\epsilon^2)$，
与从互信息的定义式出发计算得到的结果相同，
从而印证
了弱独立情形下互信息的表达式。
\end{example}

\subsection{速率函数与大偏差原理}
本小节将首先从型的概念出发，引出针对离散随机变量的大偏差原理——Sanov定理。
型的概念和Sanov定理将在我们推导拓展随机块模型的最优误差率的结论中用到。
之后我们会
介绍大偏差原理的一般形式，即Cramér定理，从而引出速率函数的概念。

\newglossaryentry{type}{name=型, description={Type}}
在信息论中，我们首先假定所有的元素均从字母集$\mathcal{X}$
中选取。
某个长度为$n$的序列$x$的\gls{type}是表示
其各字符在该序列中出现的比例，用符号$P_x$表示，
而所有的$P_x$组成集合$\P_n$。比如考虑字母集
$\mathcal{X}=\{1,2,3\}$，对于$11321$这个序列
它的型可以用三元组 $(\frac{3}{5}, 
\frac{1}{5}, \frac{1}{5})$ 表示。注意到
一个型对应着一个离散分布的概率质量函数。

假设 长度为$n$的序列$x=(x_1,\dots, x_n)$中每个元素 都是 i.i.d. 
服从分布 $Q(x)$，
则它的联合分布的概率质量函数可以记为$Q^n(x)=\prod_{i=1}^n Q(x_i)$。
对于任意长度为$n$的序列$x$，我们仍沿用上述关于$Q(x)$的定义。
为衡量$Q$和另一个不包括$Q$的概率分布的集合 $E$ 之间的距离，我们引入如下的记号：
$Q^n(E) = \sum_{x: P_x \in E \cap \P_n} Q^n(x)$。
这里 $P_x$ 是 $x$的型。

下面介绍的 Sanov 定理给出了$Q^n(E)$的上下界：
\begin{theorem}
  设 $X_1, \dots, X_n$ i.i.d. 服从分布 $Q(x)$，
  若 $E$ 表示概率分布的集合，且$E$的闭包是其自身。
  则
  \begin{equation}
  \frac{1}{(n+1)^{|\mathcal{X}|}} e^{-n D(P^*||Q)}
  \leq Q^n(E) \leq (n+1)^{|\mathcal{X}|} e^{-n D(P^*||Q)}
  \end{equation}
  其中
  \begin{equation}
    P^* = \arg\min_{P\in E} D(P||Q)
  \end{equation}
  极限情形下有：
  \begin{equation}\label{eq:limit_sanov}
    \lim_{n\to \infty} \frac{1}{n} \log Q^n(E) = -D(P^*||Q)
  \end{equation}
\end{theorem}
\begin{example}\label{ex:sanov_ldp}
若$X_1, \dots, X_n$ i.i.d. $\sim \Bern(p)$，
对于任意正数$\epsilon$，
我们可以用 Sanov 定理估计 $P(\frac{1}{n} \sum_{i=1}^n X_i > \bar{X} + \epsilon)$ 随着$n$增大收敛到零
的速率，其中$\bar{X}$是$X_1,\dots, X_n$的样本均值。
此时集合$E=\{\Bern(q)| q>p+\epsilon\}$，
而$Q^n(E)=P(\frac{1}{n} \sum_{i=1}^n X_i > \bar{X} + \epsilon)$。
因此由式\eqref{eq:limit_sanov}可得
$\lim_{n\to \infty} \frac{1}{n} \log Q^n(E)=-D(\Bern(p+\epsilon)||\Bern(p))$。
\end{example}
Sanov 定理仅针对离散型分布给出了计算诸如$P(\frac{1}{n} \sum_{i=1}^n X_i > \bar{X} + \epsilon)$
的方法，若将例 \ref{ex:sanov_ldp} 中诸$X_i$ 改为高斯分布，
则可以用Sanov 定理的推广形式 Cramér定理来解决。我们下面先介绍该定理：

\begin{theorem}\label{thm:cramer}
  \newglossaryentry{log_mgf}{name=对数矩生成函数, description={logarithmic moment generating function}}
  \newacronym{acr:log_mgf}{log-MGF}{logarithmic moment generating function}
  令 $X_1, \dots, X_n$ 为 i.i.d. 的随机变量序列，且$X_1$ 存在\gls{log_mgf} (\gls{acr:log_mgf})
$\Lambda(t)=\log \E[\exp(t X_1)]$。
令 $\Lambda^*(s)= \sup_{t \in \R} (st-\Lambda(t))$，则对于所有$s>\E[X_1]$，
我们有
\begin{equation}\label{eq:cramer}
\lim_{n\to \infty} \frac{1}{n}\log P\left(
  \sum_{i=1}^n X_i \geq  ns \right) = - \Lambda^*(s)
\end{equation}
\end{theorem}
由于$\Lambda^*(s)$描述了$P( \sum_{i=1}^n X_i \geq  ns)$随$n$增大衰减到零的速率，
\newglossaryentry{rate_function}{name=速率函数, description={rate function}}
我们把$\Lambda^*(s)$叫做\gls{rate_function}。

为方便描述$\Lambda^*(s)$和$\Lambda(s)$的关系，
我们引入凸共轭函数的概念。
\begin{definition}\label{def:convex_conjugate}
  对于定义在$\R$ 上的光滑函数 $\Lambda(s)$，
  它的凸共轭函数定义为
  $\Lambda^*(s) = \sup_{t \in \R} (st-\Lambda(t))$。
%  $\Lambda^*(s)$ 再取一次凸共轭又得到 $\Lambda(s)$自身。    
\end{definition}

\begin{example}\label{ex:cramer_ldp}
  若$X_1, \dots, X_n$ i.i.d. $\sim \mathcal{N}(0,1)$，
  对于任意正数$\epsilon$，
  下面我们 用  Cramér 定理 计算 $P(\frac{1}{n} \sum_{i=1}^n X_i > \epsilon)$ 
  随着$n$增大收敛到零
  的速率。首先计算标准正态分布的 log-MGF 为 $\Lambda(s)=s^2/2$，
  其凸共轭$\Lambda^*(s)=\Lambda(s)=s^2/2$。因此
  由式\eqref{eq:cramer}可得
  $\lim_{n\to \infty} \frac{1}{n}\log P( \sum_{i=1}^n X_i \geq  ns) =-s^2/2$。
  \end{example}
\begin{remark}
  注意到 $s^2/2=D(\mathcal{N}(s, 1) || \mathcal{N}(0,1))$。
  从某种意义上说，速率函数是KL散度的推广。
\end{remark}
\subsection{雷尼散度与切尔诺夫信息}
雷尼散度\cite{renyi1961measures}通常由雷尼熵导出，但结合我们研究的问题，
在本节中我们通过假设检验问题中的切尔诺夫信息导出我们需要的雷尼散度的形式。

在假设检验的问题中，我们要确定 i.i.d. 采样的样本
$X_1, \dots, X_n$ 服从
$P_1$ 还是 $P_2$ 分布。为此，考虑两个假设：
$H_1: Q=P_1$ 和 $H_2: Q=P_2$。
决策变量$\widehat{H}$ 的值域为 $\{P_1, P_2\}$。
在做决策的时候可能会产生两类错误，它们的概率
分别是 $\alpha_n=P(\widehat{H}=H_2|H_1)$
以及 $\beta_n=P(\widehat{H}=H_1|H_2)$。
这里下标 $n$表示错误概率随采样数量$n$而变化。
假设$P_1, P_2$的先验概率分别是 $\pi_1$ 和 $\pi_2$，
则总误差概率为：$P_e^{(n)} = \pi_1 \alpha_n
+ \pi_2 \beta_n$。

\newglossaryentry{error_exponent}{name=误差指数, description={error exponent}}
$P_e^{(n)}$ 和决策变量$\widehat{H}$的选取有关。
切尔诺夫信息指出了在渐近情形下 $P_e^{(n)}$ 所能达到的
最小值。
%更精确的说，最大的误差指数。因为
在渐近情形下 $P_e^{(n)}$ 以 $\exp(-n D^*)$ 的速率指数衰减。
我们把$D^*$叫做 \gls{error_exponent}。
$D^*$的数学定义为：
\begin{equation}
 D^* = \lim_{n\to \infty} -\frac{1}{n} \min_{\widehat{H}}
 P^{(n)}_e
\end{equation}
关于 $D^*$ 我们有如下定理：
\begin{theorem}[\cite{cover1999elements}定理11.9.1]
  \newglossaryentry{chernoff_information}{name=切尔诺夫信息, description={Chernoff Information}}
  最大误差指数 $D^*$，也称\gls{chernoff_information}， 满足
  \begin{equation}\label{eq:D_star_lambda_star}
    D^* = D(P_{\lambda^*} || P_1) = D(P_{\lambda^*}|| P_2)
  \end{equation}
  而 $P_{\lambda}$ 定义为
  \begin{equation}\label{eq:P_lambda_x}
    P_{\lambda}(x) = \frac{P^{1-\lambda}_1 (x) P^{\lambda}_2 (x)}
    {\sum_{a \in \mathcal{X}} P^{1-\lambda}_1 (a) P^{\lambda}_2 (a)}
  \end{equation}
  $\lambda^*$ 的取值使得  \eqref{eq:D_star_lambda_star} 式成立。
\end{theorem}

\newglossaryentry{not:chernoff_information}
{
  type=notation,
  name={$C(P_1||P_2)$},
  description={分布$P_1$和$P_2$之间的切尔诺夫信息}
}
为方便讨论，我们使用符号$C(P_1||P_2)$
来表示\glsdesc{not:chernoff_information}。

切尔诺夫信息的另一个等价定义是：
\begin{equation}\label{eq:C_P_1_P_2_another}
  C(P_1||P_2) = -\min_{0\leq \lambda \leq 1}
  \log \left(\sum_{x \in \mathcal{X}}
  P^{1-\lambda}_1(x)P^{\lambda}_2(x)
  \right)
\end{equation}

当我们考虑联合分布 $P_1 \times P_2$
和 $P_2 \times P_1$ 之间的  切尔诺夫信息量时，
由式 \eqref{eq:C_P_1_P_2_another}有
\begin{align*}
  C(P_1 \times P_2||P_2 \times P_1) 
  = -\min_{\lambda \in [0,1]}
  \left(\log \sum_{x\in \mathcal{X}}
  P_1^{1-\lambda}(x) P_2^{\lambda}(x) 
  +\log \sum_{y\in \mathcal{X}}
  P_2^{1-\lambda}(y) P_1^{\lambda}(y) 
  \right)
  \end{align*}
该目标函数在$\lambda=\frac{1}{2}$
处一阶导数为零而二阶导数为正，因此最小值
在$\lambda=\frac{1}{2}$取到。该最小值
即为阶数为$\frac{1}{2}$的 \gls{renyi_divergence}。
\newglossaryentry{renyi_divergence}{name=雷尼散度, description={Rényi divergence}}
记为：
\begin{equation}\label{eq:renyi_divergence}
  D_{1/2}(P_1 || P_2) = C(P_1 \times P_2||P_2 \times P_1)=
  -2\log \left(\sum_{x \in \mathcal{X}}
  \sqrt{P_1(x)P_2(x)} \right)
\end{equation}

\newglossaryentry{not:pois}
{
  type=notation,
  name={$\Pois(\lambda)$},
  description={参数为$\lambda$的泊松分布}
}
雷尼散度在刻画随机块模型及其变体（如度修正的随机块模型）的误差衰减
速率时被用到 \cite{zhang2016, gao2018community}。
比如在两社团的随机块模型中，我们取$P_1=\Pois(\frac{a}{2}),P_2=\Pois(\frac{b}{2})$
代入式 \eqref{eq:renyi_divergence} 
得
\begin{equation}\label{eq:renyi_d12_ab}
  D_{1/2}(\Pois(\frac{a}{2}) || \Pois(\frac{b}{2}))
  = \frac{1}{2}(\sqrt{a} - \sqrt{b})^2
\end{equation}

%\subsection{小节}
以上介绍的三种度量都可以看做KL散度的某种推广，在实际使用中根据所研究
的问题选取较为方便的度量形式更易获得有启发性的结论，
这一点我们将在后面的章节进行具体阐示。
